{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNApGFxx7LIVqarvTFkIELq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghat0tkach/github-triage-bot/blob/main/Github_Triage_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "!git clone https://github.com/Ghat0tkach/jlug-lenscape-event-frontend.git\n",
        "!pip install transformers torch tqdm\n",
        "!pip install groq"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rZ4id4UpSR5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt_kTQaSRjNB"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_code(code, max_length=510):\n",
        "    tokens = tokenizer.tokenize(code)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_length):\n",
        "        chunk = tokens[i:i + max_length]\n",
        "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
        "    return chunks\n",
        "\n",
        "def get_embeddings(code_chunk):\n",
        "    inputs = tokenizer(code_chunk, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "def process_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    chunks = chunk_code(content)\n",
        "    embeddings = []\n",
        "    metadata = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        embedding = get_embeddings(chunk)\n",
        "        embeddings.append(embedding)\n",
        "        metadata.append({\n",
        "            'file_path': file_path,\n",
        "            'chunk_index': i,\n",
        "            'chunk_content': chunk\n",
        "        })\n",
        "\n",
        "    return embeddings, metadata\n"
      ],
      "metadata": {
        "id": "FbIJQrXpSbZQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V23ch2DSrR2",
        "outputId": "7c69d0a2-c75a-460d-d08b-776312c1e78a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mjlug-lenscape-event-frontend\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_repository(repo_path):\n",
        "    all_embeddings = []\n",
        "    all_metadata = []\n",
        "\n",
        "    for root, _, files in os.walk(repo_path):\n",
        "        for file in tqdm(files, desc=\"Processing files\"):\n",
        "            if file.endswith(('.py', '.js', '.java', '.cpp', '.c', '.html', '.css','.jsx','.tsx','.ts')):  # Add more extensions as needed\n",
        "                print(\"File \", file)\n",
        "                file_path = os.path.join(root, file)\n",
        "                embeddings, metadata = process_file(file_path)\n",
        "                all_embeddings.extend(embeddings)\n",
        "                all_metadata.extend(metadata)\n",
        "\n",
        "    return all_embeddings, all_metadata\n",
        "\n",
        "# Process the repository\n",
        "repo_path = 'jlug-lenscape-event-frontend'  # Adjust this to the cloned repo's path\n",
        "embeddings, metadata = process_repository(repo_path)\n",
        "\n",
        "print(f\"Total embeddings generated: {len(embeddings)}\")\n",
        "print(f\"Sample embedding shape: {embeddings[0].shape}\")\n",
        "print(f\"Sample metadata: {metadata[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CgwkEbQZSdjg",
        "outputId": "6796ebaf-9d21-4d67-c673-2b7d393eb7b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  tailwind.config.ts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 10/10 [00:02<00:00,  3.34it/s]\n",
            "Processing files:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  useUserStore.ts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
            "Processing files:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  globals.css\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 1/1 [00:02<00:00,  2.57s/it]\n",
            "Processing files:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  page.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  50%|█████     | 1/2 [00:06<00:06,  6.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  layout.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]\n",
            "Processing files:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  user.ts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  50%|█████     | 1/2 [00:00<00:00,  3.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  post.ts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 2/2 [00:00<00:00,  3.86it/s]\n",
            "Processing files:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  page.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  50%|█████     | 1/2 [00:22<00:22, 22.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  layout.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 2/2 [00:22<00:00, 11.26s/it]\n",
            "Processing files:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  page.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  50%|█████     | 1/2 [00:00<00:00,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  layout.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 2/2 [00:01<00:00,  1.51it/s]\n",
            "Processing files:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  checkServer.ts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  33%|███▎      | 1/3 [00:02<00:05,  2.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  posts.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  67%|██████▋   | 2/3 [00:04<00:02,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  userApi.ts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 3/3 [00:05<00:00,  1.91s/it]\n",
            "Processing files: 100%|██████████| 2/2 [00:00<00:00, 19152.07it/s]\n",
            "Processing files:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  page.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  50%|█████     | 1/2 [00:12<00:12, 12.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  layout.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 2/2 [00:12<00:00,  6.16s/it]\n",
            "Processing files:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  page.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  50%|█████     | 1/2 [00:05<00:05,  5.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  layout.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 2/2 [00:05<00:00,  2.70s/it]\n",
            "Processing files:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  auth.utils.ts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  50%|█████     | 1/2 [00:02<00:02,  2.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  utils.ts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 2/2 [00:02<00:00,  1.44s/it]\n",
            "Processing files:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  index.ts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 1/1 [00:03<00:00,  3.30s/it]\n",
            "Processing files: 100%|██████████| 5/5 [00:00<00:00, 57614.07it/s]\n",
            "Processing files: 100%|██████████| 1/1 [00:00<00:00, 14614.30it/s]\n",
            "Processing files: 0it [00:00, ?it/s]\n",
            "Processing files: 0it [00:00, ?it/s]\n",
            "Processing files: 100%|██████████| 1/1 [00:00<00:00, 15709.00it/s]\n",
            "Processing files: 0it [00:00, ?it/s]\n",
            "Processing files: 100%|██████████| 1/1 [00:00<00:00, 15307.68it/s]\n",
            "Processing files: 0it [00:00, ?it/s]\n",
            "Processing files: 100%|██████████| 1/1 [00:00<00:00, 14413.42it/s]\n",
            "Processing files: 0it [00:00, ?it/s]\n",
            "Processing files: 100%|██████████| 1/1 [00:00<00:00, 14614.30it/s]\n",
            "Processing files: 0it [00:00, ?it/s]\n",
            "Processing files: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
            "Processing files: 0it [00:00, ?it/s]\n",
            "Processing files: 100%|██████████| 2/2 [00:00<00:00, 28339.89it/s]\n",
            "Processing files: 0it [00:00, ?it/s]\n",
            "Processing files: 100%|██████████| 13/13 [00:00<00:00, 167772.16it/s]\n",
            "Processing files: 100%|██████████| 2/2 [00:00<00:00, 25575.02it/s]\n",
            "Processing files: 100%|██████████| 9/9 [00:00<00:00, 97794.65it/s]\n",
            "Processing files:   0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  postDialogs.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:   8%|▊         | 1/12 [00:08<01:32,  8.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  faq.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  17%|█▋        | 2/12 [00:13<01:06,  6.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  teamMembersAndInvitation.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  25%|██▌       | 3/12 [00:16<00:45,  5.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  likedPosts.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  33%|███▎      | 4/12 [00:18<00:29,  3.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  googleAnalytics.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  42%|████▏     | 5/12 [00:19<00:18,  2.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  userInfoCard.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  50%|█████     | 6/12 [00:21<00:14,  2.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  participantAnalytics.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  58%|█████▊    | 7/12 [00:26<00:15,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  footer.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  67%|██████▋   | 8/12 [00:34<00:18,  4.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  loader.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  75%|███████▌  | 9/12 [00:34<00:10,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  imageModel.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  83%|████████▎ | 10/12 [00:35<00:05,  2.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  homePageHeader.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  92%|█████████▏| 11/12 [00:37<00:02,  2.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  lockedPosts.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 12/12 [00:39<00:00,  3.28s/it]\n",
            "Processing files:   0%|          | 0/13 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  alert.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:   8%|▊         | 1/13 [00:01<00:20,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  container-scroll-animation.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  15%|█▌        | 2/13 [00:04<00:28,  2.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  input.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  23%|██▎       | 3/13 [00:05<00:16,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  card.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  31%|███       | 4/13 [00:06<00:14,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  dialog.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  38%|███▊      | 5/13 [00:10<00:17,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  flip-words.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  46%|████▌     | 6/13 [00:13<00:17,  2.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  background-lines.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  54%|█████▍    | 7/13 [00:37<00:58,  9.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  toast.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  62%|██████▏   | 8/13 [00:42<00:40,  8.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  tooltip.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  69%|██████▉   | 9/13 [00:43<00:23,  5.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  hero-parallax.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  77%|███████▋  | 10/13 [00:53<00:21,  7.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  checkbox.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  85%|████████▍ | 11/13 [00:54<00:10,  5.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  button.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  92%|█████████▏| 12/13 [00:55<00:04,  4.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  label.tsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 13/13 [00:56<00:00,  4.33s/it]\n",
            "Processing files:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File  use-toast.ts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 1/1 [00:03<00:00,  4.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total embeddings generated: 158\n",
            "Sample embedding shape: (1, 768)\n",
            "Sample metadata: {'file_path': 'jlug-lenscape-event-frontend/tailwind.config.ts', 'chunk_index': 0, 'chunk_content': 'import type { Config } from \"tailwindcss\"\\n\\nconst config = {\\n  darkMode: [\"class\"],\\n  content: [\\n    \\'./pages/**/*.{ts,tsx}\\',\\n    \\'./components/**/*.{ts,tsx}\\',\\n    \\'./app/**/*.{ts,tsx}\\',\\n    \\'./src/**/*.{ts,tsx}\\',\\n\\t],\\n  prefix: \"\",\\n  theme: {\\n    container: {\\n      center: true,\\n      padding: \"2rem\",\\n      screens: {\\n        \"2xl\": \"1400px\",\\n      },\\n    },\\n    extend: {\\n      colors: {\\n        border: \"hsl(var(--border))\",\\n        input: \"hsl(var(--input))\",\\n        ring: \"hsl(var(--ring))\",\\n        background: \"hsl(var(--background))\",\\n        foreground: \"hsl(var(--foreground))\",\\n        primary: {\\n          DEFAULT: \"hsl(var(--primary))\",\\n          foreground: \"hsl(var(--primary-foreground))\",\\n        },\\n        secondary: {\\n          DEFAULT: \"hsl(var(--secondary))\",\\n          foreground: \"hsl(var(--secondary-foreground))\",\\n        },\\n        destructive: {\\n          DEFAULT: \"hsl(var(--destructive))\",\\n          foreground: \"hsl(var(--destructive-foreground))\",\\n        },\\n     '}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Save embeddings\n",
        "embeddings_array = np.array([e[0] for e in embeddings])\n",
        "np.save('embeddings.npy', embeddings_array)\n",
        "\n",
        "# Save metadata\n",
        "with open('metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "print(\"Embeddings saved to 'embeddings.npy'\")\n",
        "print(\"Metadata saved to 'metadata.json'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INCCBkhWVT58",
        "outputId": "a3889b2e-792a-46d2-99e2-821b69224954"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings saved to 'embeddings.npy'\n",
            "Metadata saved to 'metadata.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load embeddings\n",
        "loaded_embeddings = np.load('embeddings.npy')\n",
        "\n",
        "# Load metadata\n",
        "with open('metadata.json', 'r') as f:\n",
        "    loaded_metadata = json.load(f)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(loaded_embeddings)\n",
        "\n",
        "# Add metadata columns\n",
        "df['file_path'] = [item['file_path'] for item in loaded_metadata]\n",
        "df['chunk_index'] = [item['chunk_index'] for item in loaded_metadata]\n",
        "\n",
        "# Rename embedding columns\n",
        "df.columns = [f'dim_{i}' if isinstance(i, int) else i for i in df.columns]\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Display info about the DataFrame\n",
        "print(df.info())\n",
        "\n",
        "# If you want to see all columns, you can use:\n",
        "# pd.set_option('display.max_columns', None)\n",
        "# print(df)\n",
        "\n",
        "# Save as CSV if needed\n",
        "df.to_csv('embeddings_with_metadata.csv', index=False)\n",
        "print(\"Saved embeddings with metadata to 'embeddings_with_metadata.csv'\")"
      ],
      "metadata": {
        "id": "993ufnTeWdcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from scipy.spatial.distance import cosine\n",
        "from groq import Groq\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
        "\n",
        "def encode_query(query):\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()[0]\n",
        "\n",
        "# Load embeddings and metadata\n",
        "embeddings = np.load('embeddings.npy')\n",
        "with open('metadata.json', 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "def find_relevant_snippets(query, embeddings, metadata, top_k=3):\n",
        "    query_embedding = encode_query(query)\n",
        "    similarities = [1 - cosine(query_embedding, emb) for emb in embeddings]\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "    return [metadata[i] for i in top_indices], [similarities[i] for i in top_indices]\n",
        "\n",
        "# Function to query Groq API\n",
        "client = Groq(api_key=\"\")\n",
        "\n",
        "def query_groq(question, context, similarities):\n",
        "    prompt = f\"\"\"You are an AI assistant specialized in answering questions about code.\n",
        "    Given the following code snippets, their relevance scores, and a question, provide a detailed answer.\n",
        "    Use the relevance scores to weight the importance of each snippet in your answer.\n",
        "\n",
        "    Code snippets and their relevance scores:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=\"mixtral-8x7b-32768\",\n",
        "        temperature=0.5,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "# Main question-answering loop\n",
        "while True:\n",
        "    question = input(\"Ask a question about the code (or type 'exit' to quit): \")\n",
        "    if question.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    relevant_snippets, similarities = find_relevant_snippets(question, embeddings, metadata)\n",
        "    context = \"\\n\\n\".join([f\"File: {snippet['file_path']}\\nRelevance: {sim:.4f}\\nChunk: {snippet['chunk_content']}\"\n",
        "                           for snippet, sim in zip(relevant_snippets, similarities)])\n",
        "\n",
        "    answer = query_groq(question, context, similarities)\n",
        "    print(\"\\nAnswer:\", answer)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "tiRCgz3pXeJ0",
        "outputId": "6a30c3df-7b59-4e26-d247-04d08d98f5ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question about the code (or type 'exit' to quit): hi\n",
            "\n",
            "Answer: Hello! It seems like you've provided a question, but it's quite broad. I'm here to help with specific coding questions. If you have a question related to the code snippets you've given, I'd be happy to help with that. \n",
            "\n",
            "For instance, if you have a question about a function or a specific line of code in the files `jlug-lenscape-event-frontend/app/api/userApi.ts`, `jlug-lenscape-event-frontend/components/teamMembersAndInvitation.tsx`, or `jlug-lenscape-event-frontend/components/ui/container-scroll-animation.tsx`, I can provide a detailed answer based on those snippets, weighting the importance of each snippet according to their relevance scores.\n",
            "\n",
            "However, for a broad question like \"hi\", it's a bit difficult to provide a meaningful response. If you could provide more details or specify what you'd like to know about the code, I'd be happy to help!\n",
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the code (or type 'exit' to quit): can you tell what does checkServer function do?\n",
            "\n",
            "Answer: I'm sorry for the confusion, but none of the provided code snippets contain a function named `checkServer`. The relevance scores, in this case, seem to be misleading as they don't correspond to the relevance of the code snippets in relation to the question. \n",
            "\n",
            "The question asks about a function, but none of the snippets provided contain a function definition or a function call. Therefore, I'm unable to provide an answer to this question based on the given code snippets.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the code (or type 'exit' to quit): checkServer.ts\n",
            "\n",
            "Answer: Based on the relevance scores provided, none of the code snippets seem to be directly related to a file named \"checkServer.ts\". However, I can still provide some information that might be helpful.\n",
            "\n",
            "The highest relevance score (0.9889) is associated with a part of the code from \"jlug-lenscape-event-frontend/app/api/userApi.ts\". This file likely contains functions or classes related to user management in the application's API. Unfortunately, without more context, it's impossible to determine the exact purpose or functionality of this specific code snippet.\n",
            "\n",
            "The second-highest relevance score (0.9793) is associated with a code snippet from \"jlug-lenscape-event-frontend/components/teamMembersAndInvitation.tsx\". This file probably includes a React component for displaying team members and invitations. The code snippet shows a Card component from Material-UI, which typically contains some content or information.\n",
            "\n",
            "The third-highest relevance score (0.9740) is associated with a code snippet from \"jlug-lenscape-event-frontend/components/ui/container-scroll-animation.tsx\". This file likely includes a custom scroll animation for a container. The code snippet shows a motion.div component from Framer Motion, which is used for animating and transitioning elements.\n",
            "\n",
            "I hope this information helps, even though it does not directly answer your question about \"checkServer.ts\". If you have any other questions or need more information about these code snippets, please let me know!\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-267e60dc2be0>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Main question-answering loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask a question about the code (or type 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-DuSERLUYdco"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}